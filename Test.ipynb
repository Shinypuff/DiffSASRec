{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac56000c-aa6d-48e1-80e7-1d9bef03069d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from SASRec.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "241a912b-f5b7-4685-8686-92f78af22d8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from SASRec.model import SASRec, SASRecWithDiffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18bc915c-1bea-4da9-a8ae-6aa7c06b7392",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir('SASRec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ea8cc79-920a-401c-a434-6ab2dd906b73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = pd.Series(\n",
    "    dict(\n",
    "        dataset='ml-1m',\n",
    "        train_dir='default',\n",
    "        maxlen=200,\n",
    "        dropout_rate=0.2,\n",
    "        batch_size=128,\n",
    "        device='cpu',\n",
    "        hidden_units=50,\n",
    "        lr=0.001,\n",
    "        num_blocks=2,\n",
    "        num_epochs=1000,\n",
    "        num_heads=1,\n",
    "        l2_emb=0.0,\n",
    "        state_dict_path=None,\n",
    "        n_recs=20,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ecb101a-84ee-4645-a0e3-6cb7e84c275a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "u2i_index, i2u_index = build_index(args.dataset)\n",
    "dataset = data_partition(args.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a0a8601-5337-4d72-9383-6ea1304f61d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average sequence length: 163.50\n"
     ]
    }
   ],
   "source": [
    "[user_train, user_valid, user_test, usernum, itemnum] = dataset\n",
    "num_batch = (len(user_train) - 1) // args.batch_size + 1\n",
    "\n",
    "cc = 0.0\n",
    "for u in user_train:\n",
    "    cc += len(user_train[u])\n",
    "\n",
    "print('average sequence length: %.2f' % (cc / len(user_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85bd8cfe-0b63-4abd-b2af-cebf38cd08f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open(os.path.join(args.dataset + '_' + args.train_dir, 'log.txt'), 'w')\n",
    "f.write('epoch (val_ndcg, val_hr) (test_ndcg, test_hr)\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3e010e4-7786-414a-bcc0-24dee2917fd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sampler = WarpSampler(user_train, usernum, itemnum, batch_size=args.batch_size, maxlen=args.maxlen, n_workers=3)\n",
    "model = SASRec(usernum, itemnum, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75a5e5e-d08a-429f-b225-85d215ac1d01",
   "metadata": {},
   "source": [
    "# Training Vanilla SASRec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bca7ebd2-e3ce-4bc1-8c31-7473f82c35c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    try:\n",
    "        torch.nn.init.xavier_normal_(param.data)\n",
    "    except:\n",
    "        pass # just ignore those failed init layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65e5a18e-a7c2-409d-8e7c-b877f73e4ce6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.pos_emb.weight.data[0, :] = 0\n",
    "model.item_emb.weight.data[0, :] = 0\n",
    "\n",
    "model.train() # enable model training\n",
    "\n",
    "epoch_start_idx = 1\n",
    "if args.state_dict_path is not None:\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(args.state_dict_path, map_location=torch.device(args.device)))\n",
    "        tail = args.state_dict_path[args.state_dict_path.find('epoch=') + 6:]\n",
    "        epoch_start_idx = int(tail[:tail.find('.')]) + 1\n",
    "    except: # in case your pytorch version is not 1.6 etc., pls debug by pdb if load weights failed\n",
    "        print('failed loading state_dicts, pls check file path: ', end=\"\")\n",
    "        print(args.state_dict_path)\n",
    "        print('pdb enabled for your quick check, pls type exit() if you do not need it')\n",
    "        import pdb; pdb.set_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c23230f-793b-434d-925c-ed7871d19360",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bce_criterion = torch.nn.BCEWithLogitsLoss() # torch.nn.BCELoss()\n",
    "adam_optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, betas=(0.9, 0.98))\n",
    "\n",
    "best_val_ndcg, best_val_hr = 0.0, 0.0\n",
    "best_test_ndcg, best_test_hr = 0.0, 0.0\n",
    "T = 0.0\n",
    "t0 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2602083-2f8d-4a81-8d7c-1f62080283b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss in epoch 1 iteration 0: 1.3935099840164185\n",
      "loss in epoch 1 iteration 1: 1.3839380741119385\n",
      "loss in epoch 1 iteration 2: 1.3789432048797607\n",
      "loss in epoch 1 iteration 3: 1.3697421550750732\n",
      "loss in epoch 1 iteration 4: 1.3589098453521729\n",
      "loss in epoch 1 iteration 5: 1.3480799198150635\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m bce_criterion(neg_logits[indices], neg_labels[indices])\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mitem_emb\u001b[38;5;241m.\u001b[39mparameters(): loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39ml2_emb \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mnorm(param)\n\u001b[1;32m---> 14\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     15\u001b[0m adam_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss in epoch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m iteration \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch, step, loss\u001b[38;5;241m.\u001b[39mitem())) \u001b[38;5;66;03m# expected 0.4~0.6 after init few epochs\u001b[39;00m\n",
      "File \u001b[1;32mF:\\Anaconda\\Lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mF:\\Anaconda\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epoch_start_idx, args.num_epochs + 1):\n",
    "    for step in range(num_batch):\n",
    "        u, seq, pos, neg = sampler.next_batch() # tuples to ndarray\n",
    "        u, seq, pos, neg = np.array(u), np.array(seq), np.array(pos), np.array(neg)\n",
    "        \n",
    "        pos_logits, neg_logits = model(u, seq, pos, neg)\n",
    "        pos_labels, neg_labels = torch.ones(pos_logits.shape, device=args.device), torch.zeros(neg_logits.shape, device=args.device)\n",
    "\n",
    "        adam_optimizer.zero_grad()\n",
    "        indices = np.where(pos != 0)\n",
    "        loss = bce_criterion(pos_logits[indices], pos_labels[indices])\n",
    "        loss += bce_criterion(neg_logits[indices], neg_labels[indices])\n",
    "        for param in model.item_emb.parameters(): loss += args.l2_emb * torch.norm(param)\n",
    "        loss.backward()\n",
    "        adam_optimizer.step()\n",
    "        print(\"loss in epoch {} iteration {}: {}\".format(epoch, step, loss.item())) # expected 0.4~0.6 after init few epochs\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        model.eval()\n",
    "        t1 = time.time() - t0\n",
    "        T += t1\n",
    "        print('Evaluating', end='')\n",
    "        t_test = evaluate(model, dataset, args)\n",
    "        t_valid = evaluate_valid(model, dataset, args)\n",
    "        print('epoch:%d, time: %f(s), valid (NDCG@10: %.4f, HR@10: %.4f), test (NDCG@10: %.4f, HR@10: %.4f)'\n",
    "                % (epoch, T, t_valid[0], t_valid[1], t_test[0], t_test[1]))\n",
    "\n",
    "        if t_valid[0] > best_val_ndcg or t_valid[1] > best_val_hr or t_test[0] > best_test_ndcg or t_test[1] > best_test_hr:\n",
    "            best_val_ndcg = max(t_valid[0], best_val_ndcg)\n",
    "            best_val_hr = max(t_valid[1], best_val_hr)\n",
    "            best_test_ndcg = max(t_test[0], best_test_ndcg)\n",
    "            best_test_hr = max(t_test[1], best_test_hr)\n",
    "            folder = args.dataset + '_' + args.train_dir\n",
    "            fname = 'SASRec.epoch={}.lr={}.layer={}.head={}.hidden={}.maxlen={}.pth'\n",
    "            fname = fname.format(epoch, args.lr, args.num_blocks, args.num_heads, args.hidden_units, args.maxlen)\n",
    "            torch.save(model.state_dict(), os.path.join(folder, fname))\n",
    "\n",
    "        f.write(str(epoch) + ' ' + str(t_valid) + ' ' + str(t_test) + '\\n')\n",
    "        f.flush()\n",
    "        t0 = time.time()\n",
    "        model.train()\n",
    "\n",
    "    if epoch == args.num_epochs:\n",
    "        folder = args.dataset + '_' + args.train_dir\n",
    "        fname = 'SASRec.epoch={}.lr={}.layer={}.head={}.hidden={}.maxlen={}.pth'\n",
    "        fname = fname.format(args.num_epochs, args.lr, args.num_blocks, args.num_heads, args.hidden_units, args.maxlen)\n",
    "        torch.save(model.state_dict(), os.path.join(folder, fname))\n",
    "\n",
    "f.close()\n",
    "sampler.close()\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e903d0-76fa-4bfe-a53e-7aa181c05184",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8668d1db-d46e-4c0a-9e6c-41b723f375cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "proposed_items = np.arange(itemnum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f8c127b-c40e-4069-a81b-8459418ba2a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  1.8008,  -6.6977,  -7.7460,  ...,  -4.7461,  -6.7682,  -8.1332],\n",
       "        [  1.4729,   1.3699,  -5.8549,  ...,  -7.9728,   1.2932,   5.8851],\n",
       "        [  1.6071,   0.1935,  -9.1936,  ...,  -7.9932,  -9.3019, -10.7897],\n",
       "        ...,\n",
       "        [  0.7931,   2.8766,  -1.7839,  ...,  -4.6640,  -5.1508,   1.6283],\n",
       "        [  1.1645,  -4.0154,  -2.8742,  ...,  -3.4040,  -6.1271,  -8.6607],\n",
       "        [  1.9209,   3.1261,  -4.4585,  ..., -14.5340,  -0.1543,   0.5881]],\n",
       "       grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(u, seq, proposed_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2da6938-7e21-4272-bd75-af85bb9ea943",
   "metadata": {},
   "source": [
    "# Diffusin SASRec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "479a44de-ef2d-434c-a736-d2ceab93f7cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = SASRecWithDiffusion(usernum, itemnum, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e9be58ef-0822-43b1-8b79-e3225d6c4160",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sampler = WarpSampler(user_train, usernum, itemnum, batch_size=args.batch_size, maxlen=args.maxlen + args.n_recs, n_workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9391a8a3-2179-4ebc-98cd-21509c6edf08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "adam_optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, betas=(0.9, 0.98))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a8f7946-4e00-4d64-9ce9-ba06f33fa09c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss in epoch 1 iteration 0: 18.688432693481445\n",
      "loss in epoch 1 iteration 1: 18.77422523498535\n",
      "loss in epoch 1 iteration 2: 18.20121192932129\n",
      "loss in epoch 1 iteration 3: 17.8505916595459\n",
      "loss in epoch 1 iteration 4: 18.061403274536133\n",
      "loss in epoch 1 iteration 5: 18.004785537719727\n",
      "loss in epoch 1 iteration 6: 17.839412689208984\n",
      "loss in epoch 1 iteration 7: 17.895626068115234\n",
      "loss in epoch 1 iteration 8: 17.905954360961914\n",
      "loss in epoch 1 iteration 9: 16.981290817260742\n",
      "loss in epoch 1 iteration 10: 17.117555618286133\n",
      "loss in epoch 1 iteration 11: 17.530784606933594\n",
      "loss in epoch 1 iteration 12: 17.30600357055664\n",
      "loss in epoch 1 iteration 13: 17.533531188964844\n",
      "loss in epoch 1 iteration 14: 17.224214553833008\n",
      "loss in epoch 1 iteration 15: 16.459470748901367\n",
      "loss in epoch 1 iteration 16: 16.650836944580078\n",
      "loss in epoch 1 iteration 17: 16.868009567260742\n",
      "loss in epoch 1 iteration 18: 16.31648826599121\n",
      "loss in epoch 1 iteration 19: 16.015810012817383\n",
      "loss in epoch 1 iteration 20: 16.50954246520996\n",
      "loss in epoch 1 iteration 21: 15.99111557006836\n",
      "loss in epoch 1 iteration 22: 15.670860290527344\n",
      "loss in epoch 1 iteration 23: 15.565495491027832\n",
      "loss in epoch 1 iteration 24: 16.075756072998047\n",
      "loss in epoch 1 iteration 25: 15.529764175415039\n",
      "loss in epoch 1 iteration 26: 15.666546821594238\n",
      "loss in epoch 1 iteration 27: 15.477301597595215\n",
      "loss in epoch 1 iteration 28: 15.519768714904785\n",
      "loss in epoch 1 iteration 29: 15.333778381347656\n",
      "loss in epoch 1 iteration 30: 15.073174476623535\n",
      "loss in epoch 1 iteration 31: 15.425126075744629\n",
      "loss in epoch 1 iteration 32: 15.084650993347168\n",
      "loss in epoch 1 iteration 33: 14.817877769470215\n",
      "loss in epoch 1 iteration 34: 14.930130004882812\n",
      "loss in epoch 1 iteration 35: 14.573458671569824\n",
      "loss in epoch 1 iteration 36: 15.432985305786133\n",
      "loss in epoch 1 iteration 37: 14.68811321258545\n",
      "loss in epoch 1 iteration 38: 14.36473274230957\n",
      "loss in epoch 1 iteration 39: 14.834593772888184\n",
      "loss in epoch 1 iteration 40: 14.751301765441895\n",
      "loss in epoch 1 iteration 41: 14.500262260437012\n",
      "loss in epoch 1 iteration 42: 14.1277437210083\n",
      "loss in epoch 1 iteration 43: 14.490882873535156\n",
      "loss in epoch 1 iteration 44: 14.140687942504883\n",
      "loss in epoch 1 iteration 45: 13.855530738830566\n",
      "loss in epoch 1 iteration 46: 13.980137825012207\n",
      "loss in epoch 1 iteration 47: 14.134228706359863\n",
      "loss in epoch 2 iteration 0: 13.911872863769531\n",
      "loss in epoch 2 iteration 1: 13.650873184204102\n",
      "loss in epoch 2 iteration 2: 13.634737014770508\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_loss(seq)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mitem_emb\u001b[38;5;241m.\u001b[39mparameters(): loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39ml2_emb \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mnorm(param)\n\u001b[1;32m---> 11\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     12\u001b[0m adam_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss in epoch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m iteration \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch, step, loss\u001b[38;5;241m.\u001b[39mitem()))\n",
      "File \u001b[1;32mF:\\Anaconda\\Lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mF:\\Anaconda\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epoch_start_idx, args.num_epochs + 1):\n",
    "    for step in range(num_batch):\n",
    "        u, seq, pos, neg = sampler.next_batch() # tuples to ndarray\n",
    "        u, seq, pos, neg = np.array(u), np.array(seq), np.array(pos), np.array(neg)\n",
    "\n",
    "        adam_optimizer.zero_grad()\n",
    "        indices = np.where(pos != 0)\n",
    "        loss = model.get_loss(seq)\n",
    "        \n",
    "        for param in model.item_emb.parameters(): loss += args.l2_emb * torch.norm(param)\n",
    "        loss.backward()\n",
    "        adam_optimizer.step()\n",
    "        \n",
    "        print(\"loss in epoch {} iteration {}: {}\".format(epoch, step, loss.item())) # expected 0.4~0.6 after init few epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaae1576-c9dd-48fc-93da-875c191bed65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
